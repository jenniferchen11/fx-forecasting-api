{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79aVx39DNA9R"
      },
      "source": [
        "Training recurrent neural network (RNN) models to predict bilateral foreign exchange rates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JMwqxEANEFc"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6clHIeENZzw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import optuna\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "import json\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EAvj0RdNiEk"
      },
      "outputs": [],
      "source": [
        "'''Hyperparameters'''\n",
        "lookback = 365 * 2\n",
        "learning_rate = 0.001\n",
        "batch_size = 512\n",
        "num_lstm_layers = 3\n",
        "num_lstm_units = 100\n",
        "dropout_rate = 0.2\n",
        "loss_func = nn.MSELoss()\n",
        "weight_decay = 0.0\n",
        "num_epochs = 50\n",
        "percent_training_data = 0.9\n",
        "scale = True\n",
        "\n",
        "# options: 'lstm', 'gru'\n",
        "model_architecture = 'lstm'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZzLAxuvNjBx"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlR_CxwWOvRp"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_num_cols, hidden_size, num_layers=1, device='cpu'):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        # add lstm layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_num_cols,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            dropout=dropout_rate,\n",
        "            batch_first=True)\n",
        "        # add fully-connected linear layer\n",
        "        self.fc = nn.Linear(hidden_size, input_num_cols)\n",
        "        self.device=device\n",
        "\n",
        "    def forward(self, x):\n",
        "      # hidden state\n",
        "      h0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(self.device)\n",
        "      #initial cell state\n",
        "      c0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(self.device)\n",
        "      out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "      # filter out everything except the last timestep’s output\n",
        "      out = self.fc(out[:, -1, :])\n",
        "      return out\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_num_cols, hidden_size, num_layers=1, device='cpu'):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        # add gru layers\n",
        "        self.gru = nn.GRU(\n",
        "            input_num_cols,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            dropout=0.2,\n",
        "            batch_first=True)\n",
        "        # add fully-connected linear layer\n",
        "        self.fc = nn.Linear(hidden_size, input_num_cols)\n",
        "\n",
        "        self.device=device\n",
        "\n",
        "    def forward(self, x):\n",
        "      # hidden state\n",
        "      h0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(self.device)\n",
        "      out, _ = self.gru(x, h0)\n",
        "\n",
        "      # filter out everything except the last timestep’s output\n",
        "      out = self.fc(out[:, -1, :])\n",
        "      return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFX2ej6AN8TB"
      },
      "source": [
        "# Shared Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PCOkBK2OASv"
      },
      "outputs": [],
      "source": [
        "'''Helper Functions'''\n",
        "def add_multiindex(forex_df):\n",
        "    mi_df = forex_df.set_index(['slug', 'date'])\n",
        "\n",
        "    new_index = pd.MultiIndex.from_product([mi_df.index.levels[0], mi_df.index.levels[1]], names=['slug', 'date'])\n",
        "    mi_df = mi_df.reindex(new_index, fill_value=None).bfill()\n",
        "    mi_df = mi_df.sort_index()\n",
        "\n",
        "    return mi_df\n",
        "\n",
        "def convert_to_np(mi_df, num_cols, num_dates):\n",
        "    df = mi_df.reset_index().set_index(['slug', 'date']).unstack(level='date')\n",
        "    data_np = df.to_numpy()\n",
        "\n",
        "    # shape for 2D array\n",
        "    shape = (num_cols, num_dates)\n",
        "\n",
        "    # reshape the array to 3D\n",
        "    data_np = data_np.reshape(shape).astype(np.float32)\n",
        "    data_np = data_np.transpose()\n",
        "    print(f\"Data shape: {data_np.shape}\")\n",
        "    return data_np\n",
        "\n",
        "def get_generators(data_np):\n",
        "    data_np = np.transpose(data_np)\n",
        "    data_r = data_np[:, ::-1]\n",
        "    data_np = np.concatenate((data_np, data_r), axis=1)\n",
        "    data_np = np.tile(data_np, 10)\n",
        "    data_np = np.transpose(data_np)\n",
        "\n",
        "    # create the features\n",
        "    x_indices = np.arange(lookback)[:, None] + np.arange(len(data_np) - lookback)\n",
        "    x = data_np[x_indices]\n",
        "    x = np.transpose(x, (1, 0, 2))\n",
        "\n",
        "    # create the labels\n",
        "    y = data_np[lookback:]\n",
        "    print(f\"x shape: {x.shape}\")\n",
        "    print(f\"y shape: {y.shape}\")\n",
        "\n",
        "    train_size = int(percent_training_data * x.shape[0])\n",
        "\n",
        "    global train_data_x\n",
        "    global train_data_y\n",
        "    global test_data_x\n",
        "    global test_data_y\n",
        "\n",
        "    train_data_x = x[:train_size, :]\n",
        "    train_data_y = y[:train_size, :]\n",
        "\n",
        "    test_data_x = x[train_size:, :]\n",
        "    test_data_y = y[train_size:, :]\n",
        "\n",
        "    print(f\"Training set shape: {train_data_x.shape}\")\n",
        "    print(f\"Test set shape: {test_data_x.shape}\")\n",
        "\n",
        "    train_generator = DataLoader(TensorDataset(torch.from_numpy(train_data_x), torch.from_numpy(train_data_y)), batch_size=batch_size, shuffle=False)\n",
        "    test_generator = DataLoader(TensorDataset(torch.from_numpy(test_data_x), torch.from_numpy(test_data_y)), batch_size=1, shuffle=False)\n",
        "\n",
        "    return train_generator, test_generator\n",
        "\n",
        "def fit_and_predict(model, train_generator, test_generator):\n",
        "    min_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for data in train_generator:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # clear the old gradients\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "    model.eval()\n",
        "    prediction_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(test_generator):\n",
        "          if i == 0:\n",
        "            inputs, _ = data\n",
        "            prediction_list = copy.deepcopy(inputs.numpy()).reshape(lookback, 1)\n",
        "            inputs = inputs.to(device)\n",
        "          else:\n",
        "            inputs =copy.deepcopy(prediction_list[-lookback:])\n",
        "            inputs = inputs.reshape((1, lookback, num_cols))\n",
        "            inputs = torch.from_numpy(inputs).to(device)\n",
        "          output = model(inputs).cpu().numpy()\n",
        "          prediction_list = np.append(prediction_list, output, axis=0)\n",
        "\n",
        "    prediction_list = prediction_list[lookback:]\n",
        "    return prediction_list\n",
        "\n",
        "def save_model(model, currency_pair):\n",
        "    currency_pair = currency_pair.replace('/', '_')\n",
        "    dir_models = directory + 'trained/'\n",
        "    files = set(os.listdir(dir_models))\n",
        "\n",
        "    i = 0\n",
        "    while(True):\n",
        "        fpath = currency_pair + str(i) + '.pt'\n",
        "        if fpath not in files:\n",
        "            break\n",
        "        i += 1\n",
        "    full_path = dir_models + fpath\n",
        "\n",
        "    torch.save(model.state_dict(), full_path)\n",
        "\n",
        "    metadata = {\n",
        "       'num_epochs': num_epochs,\n",
        "       'lookback': lookback,\n",
        "       'learning_rate': learning_rate,\n",
        "       'batch_size': batch_size,\n",
        "       'num_lstm_layers': num_lstm_layers,\n",
        "       'num_lstm_units': num_lstm_units,\n",
        "       'dropout_rate': dropout_rate,\n",
        "       'weight_decay': weight_decay,\n",
        "       'scale': scale,\n",
        "       'model_architecture': model_architecture,\n",
        "    }\n",
        "\n",
        "    metadata_fpath = directory + 'model_metadata/' + currency_pair + str(i)\n",
        "\n",
        "    # write dictionary to json file\n",
        "    with open(metadata_fpath, 'w') as json_file:\n",
        "        json.dump(metadata, json_file)\n",
        "\n",
        "def check_if_completed(currency_pair):\n",
        "    currency_pair = currency_pair.replace('/', '_')\n",
        "    dir_models = directory + 'trained/'\n",
        "\n",
        "    files = set(os.listdir(dir_models))\n",
        "\n",
        "    for file in files:\n",
        "        fname = currency_pair + str(0)\n",
        "        if fname in file:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def generate_sine_wave_array(length, wave_frequency):\n",
        "    x = np.arange(length)\n",
        "    y = np.sin(2 * np.pi * wave_frequency * x)\n",
        "    return y\n",
        "\n",
        "def generate_monotonically_increasing_array(length):\n",
        "    x = np.arange(length)\n",
        "    x = x/1000\n",
        "    return x\n",
        "\n",
        "def get_forex_df(fname, scaler):\n",
        "  forex_df = pd.read_csv(fname, usecols=['slug', 'date', 'high', 'low', 'currency'])\n",
        "\n",
        "  if filter_to_one:\n",
        "    forex_df = forex_df[forex_df['slug'].isin(['USD/MWK'])]\n",
        "    forex_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  forex_df['date'] = pd.to_datetime(forex_df['date'])\n",
        "  forex_df.sort_values(by=['slug', 'date'], ascending=True, inplace=True)\n",
        "\n",
        "  avgs = pd.DataFrame()\n",
        "  avgs['rate'] = (forex_df['high'] + forex_df['low'])/2\n",
        "  forex_df = pd.concat([avgs, forex_df], axis=1)\n",
        "\n",
        "  # monotonically-increasing dummy data (for testing purposes)\n",
        "  if use_dummy_data == 1:\n",
        "    dummy = generate_monotonically_increasing_array(len(forex_df))\n",
        "    dummy = pd.Series(dummy).reset_index(drop=True)\n",
        "    forex_df['rate'] = dummy\n",
        "\n",
        "  # wave-shaped dummy data (for testing purposes)\n",
        "  elif use_dummy_data == 2:\n",
        "    dummy = generate_sine_wave_array(len(forex_df), 1/10)\n",
        "    dummy = pd.Series(dummy).reset_index(drop=True)\n",
        "    forex_df['rate'] = dummy\n",
        "\n",
        "  unscaled_rates = copy.deepcopy(forex_df['rate'])\n",
        "\n",
        "  if scale:\n",
        "    scaled = scaler.fit(forex_df[['rate']])\n",
        "    scaled_rates = scaler.transform(forex_df[['rate']])\n",
        "    forex_df['rate'] = scaled_rates[:, 0]\n",
        "\n",
        "  forex_df = forex_df.drop('high', axis=1)\n",
        "  forex_df = forex_df.drop('low', axis=1)\n",
        "  return forex_df, unscaled_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EigrOI9ERyV7"
      },
      "outputs": [],
      "source": [
        "'''Global Vars'''\n",
        "\n",
        "directory = '/content/gdrive/MyDrive/Colab Notebooks/exchange_rates/'\n",
        "train_data_x, train_data_y, test_data_x, test_data_y = None, None, None, None\n",
        "filter_to_one = None\n",
        "use_dummy_data = 0\n",
        "num_cols = 1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnVQUkL_O3OM"
      },
      "source": [
        "# Approach 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwtvdMCbvG0m"
      },
      "source": [
        "Approach 1: Creating one model and feeding all exchange rates into it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLqJZfaOvGJG"
      },
      "outputs": [],
      "source": [
        "def instantiate_loss_and_opt(num_cols):\n",
        "    criterion = loss_func\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    return criterion, optimizer\n",
        "\n",
        "\n",
        "'''__main__ '''\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    fname = directory + \"forex_filtered.csv\"\n",
        "    forex_df, unscaled_rates = get_forex_df(fname, scaler)\n",
        "\n",
        "    currency_pairs = forex_df['slug'].unique()\n",
        "    # currency_pairs = forex_df['currency'].unique()\n",
        "\n",
        "    num_prediction = 30\n",
        "    forecast = None\n",
        "\n",
        "    model = LSTM(num_cols, num_lstm_units, num_lstm_layers, 'cuda')\n",
        "    if model_architecture == 'gru':\n",
        "        model = GRU(num_cols, num_lstm_units, num_lstm_layers)\n",
        "    model.to(device)\n",
        "\n",
        "    for currency_pair in currency_pairs:\n",
        "        if check_if_completed(currency_pair) == False:\n",
        "            print(f\"Beginning training for: {currency_pair}\")\n",
        "            df = forex_df[forex_df['slug'] == currency_pair]\n",
        "            df = df[['rate', 'date', 'slug']]\n",
        "\n",
        "            mi_df = add_multiindex(df)\n",
        "\n",
        "            num_cols = len(df['slug'].unique())\n",
        "            num_dates = len(mi_df.index.levels[1])\n",
        "\n",
        "            data_np = convert_to_np(mi_df, num_cols, num_dates)\n",
        "            train_generator, test_generator = get_generators(data_np)\n",
        "\n",
        "            criterion, optimizer = instantiate_loss_and_opt(num_cols)\n",
        "            predictions = fit_and_predict(model, train_generator, test_generator)\n",
        "\n",
        "            save_model(model, currency_pair)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzzBZdSsO7YU"
      },
      "source": [
        "# Approach 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKHoWMAzu38P"
      },
      "source": [
        "Approach 2: Creating a separate model for each currency pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POtrK5IlnkPu"
      },
      "outputs": [],
      "source": [
        "def instantiate_model(num_cols):\n",
        "    model = LSTM(num_cols, num_lstm_units, num_lstm_layers, device)\n",
        "    if model_architecture == 'gru':\n",
        "        model = GRU(num_cols, num_lstm_units, num_lstm_layers)\n",
        "    model.to(device)\n",
        "    criterion = loss_func\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    return model, criterion, optimizer\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using: {device}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    fname = directory + \"forex_filtered.csv\"\n",
        "    forex_df, unscaled_rates = get_forex_df(fname, scaler)\n",
        "\n",
        "    currency_pairs = forex_df['slug'].unique()\n",
        "\n",
        "    num_prediction = 30\n",
        "    forecast = None\n",
        "\n",
        "    for currency_pair in currency_pairs:\n",
        "        if check_if_completed(currency_pair) == False:\n",
        "            print(f\"Beginning training for: {currency_pair}\")\n",
        "            df = forex_df[forex_df['slug'] == currency_pair]\n",
        "            df = df[['rate', 'date', 'slug']]\n",
        "\n",
        "            mi_df = add_multiindex(df)\n",
        "\n",
        "            num_cols = len(df['slug'].unique())\n",
        "            num_dates = len(mi_df.index.levels[1])\n",
        "\n",
        "            data_np = convert_to_np(mi_df, num_cols, num_dates)\n",
        "            train_generator, test_generator = get_generators(data_np)\n",
        "\n",
        "            model, criterion, optimizer = instantiate_model(num_cols)\n",
        "            predictions = fit_and_predict(model, train_generator, test_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlJomrn5PT0f"
      },
      "source": [
        "# Displaying Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a8th1iFMu4N"
      },
      "outputs": [],
      "source": [
        "def forecast_future_sequence(num_prediction, model, input_data, num_cols, lookback):\n",
        "\n",
        "    prediction_list = input_data[-lookback:]\n",
        "    prediction_list = prediction_list.reshape((lookback, 1))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_prediction):\n",
        "            x = copy.deepcopy(prediction_list[-(lookback):])\n",
        "            x = x.reshape((1, lookback, num_cols))\n",
        "            x = torch.from_numpy(x)\n",
        "            x = x.to(torch.float32).to(device)\n",
        "            output = model(x)\n",
        "            prediction_list = np.append(prediction_list, output.cpu().numpy(), axis=0)\n",
        "        prediction_list = prediction_list[lookback:]\n",
        "\n",
        "    return prediction_list\n",
        "\n",
        "def calculate_mse(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "\n",
        "def graph_predicted_and_actual_historical(predictions, test_data_y):\n",
        "    p = predictions[-365:]\n",
        "    actual = test_data_y[-365:]\n",
        "\n",
        "    time_steps = [i for i in range(len(p))]\n",
        "    plt.plot(time_steps, p, color='blue', label='Predicted Rates')\n",
        "    plt.plot(time_steps, actual, color='red', label='Historical Rates')\n",
        "\n",
        "    plt.xlabel('Day #')\n",
        "    plt.ylabel('Exchange Rate')\n",
        "    plt.title('Predicted Values')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def graph_forecasted_sequence(num_prediction, model, lookback, forex_df):\n",
        "    '''Displaying Forecasted Future Sequence'''\n",
        "\n",
        "    num_cols = 1\n",
        "    initial_lookback = forex_df['rate'][-lookback:].to_numpy()\n",
        "\n",
        "    forecast = forecast_future_sequence(num_prediction, model, initial_lookback, num_cols, lookback)\n",
        "\n",
        "    time_steps = [i for i in range(len(initial_lookback) + len(forecast))]\n",
        "    plt.plot(time_steps[:len(initial_lookback)], initial_lookback, label='Past', color='blue')\n",
        "\n",
        "    plt.plot(time_steps[len(initial_lookback):], forecast[:, 0], label='Predicted', color='red')\n",
        "\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Values')\n",
        "    plt.title('Actual vs. Predicted Values')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def graph_historical(forex_df):\n",
        "    historical = forex_df['rate']\n",
        "    time_steps = [i for i in range(len(historical))]\n",
        "\n",
        "    plt.plot(time_steps, historical, label='Historical', color='red')\n",
        "\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Values')\n",
        "    plt.title('Actual vs. Predicted Values')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
